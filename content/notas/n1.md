---
title: "Sobre R2"
date: 2025-01-15
description: "data sintética"
showReadingTime: false
# showDate: false
---


Sean $$\pmb{q}=(\cdots,q_k,\cdots)$$ con $k\in I$, ciertos parámetros que definen un predictor del tipo $f(\pmb{q},\pmb{x})=\sum_{k\in I}q_kf_k(\pmb{x})$, una función de costo $\varphi(\pmb{q})$, definida a partir de ciertos pares de puntos $\{(\pmb{x}_i,y_i)\}_{i\in I_N}$ que forman los datos de entrenamiento.

Sean los $m$ parámetros $\pmb{q}^*=(\cdots,q_k^*,\cdots)$ con $k\in I$ corresponden al mínimo de $\varphi$.

### data sintética

Supongamos que las "abscisas" de los datos de entrenamiento $\pmb{x}_i$ se generan siguiendo cierta distribución de probabilidad $P_{\pmb{X}}$, además de cierta variable aditiva $\epsilon$ que sigue una distribución de probabilidad $P_{\xi}$, los parámetros objetivo $\pmb{q}'$ se generan siguiendo una distribución $P_{\pmb{Q}}$, por último las "ordenadas" de los datos de entrenamiento $y_i$ se obtienen a usando además el predictor según
$y_i=\sum_{k\in I}q_k' f_k(\pmb{x}_i)+\epsilon$

### $R^2$ coeficiente de correlación

$R^{2}=\dfrac{V(\hat{Y})}{V(Y)}$, pensadas en términos de variables aleatorias que generan los datos $Y=\sum_{k\in I}Q_kf_k(\pmb{X})+\xi$, mientras que el estadístico asociado a la predicción  $\hat{Y}=\sum_{k\in I}q_k^*f_k(\pmb{X})$.

Dado que las variables $Q_k$,  $f_k(\pmb{X})$ y  $\xi$ son independientes $V(Y)=\sum_{k\in I}V\pmb{(}Q_kf_k(\pmb{X})\pmb{)}+V(\xi)$ y $V(\hat{Y})=\sum_{k\in I}q_k^*V\pmb{(}f_k(\pmb{X})\pmb{)}$.

En particular $V\pmb{(}Q_kf_k(\pmb{X})\pmb{)}=E_{\pmb{Q}}\pmb{(}Q_k^2\pmb{)}E_{\pmb{X}}\pmb{(}f_k^2(\pmb{X})\pmb{)}-[E_{\pmb{Q}}\pmb{(}Q_k\pmb{)}E_{\pmb{X}}\pmb{(}f_k(\pmb{X})\pmb{)}]^2$.

Vamos al caso particular considero una sola variable independiente $X\sim U(0,1)$ construyo las $N$ entradas $\{x_i\}_{i\in I_N}$, los parámetros objetivo del entrenamiento provienen de las variables aleatorias $Q_k\sim U(0,1)$ con los que genero $\{q_k'\}_{k\in I}$, $\xi\sim U(-\epsilon,\epsilon)$ y $f_k(x)=\mathtt{sen}(2\pi k x)$.

* $E_{\pmb{Q}}(Q_k^2)=\int_{0}^1 q_k^2dq_k=1/3$
* $E_{\pmb{Q}}(Q_k)=\int_{0}^1 q_kdq_k=1/2$
* $E_{X}\pmb{(}f_k^2(X)\pmb{)}=\int_{0}^1 \mathtt{sen}^2(2\pi k x) dx=\frac{x}{2}|_0^1-{\frac {1}{8\pi k}}\mathtt{sin}(4\pi k x)|_0^1=1/2$
* $E_{X}\pmb{(}f_k(X)\pmb{)}=\int_{0}^1 \mathtt{sen}(2\pi k x) dx=-\mathtt{cos}(2\pi k x)/2\pi k|_0^1=(1-\mathtt{cos}(2\pi k))/2\pi k=0$
* $V(\xi)=\epsilon^2/3$

$V(Y)=\sum_{k\in I}V\pmb{(}Q_kf_k(\pmb{X})\pmb{)}+V(\xi)=m/6+\epsilon^2/3$, $V(\hat{Y})=\sum_{k\in I}q_k^*V\pmb{(}f_k(\pmb{X})\pmb{)}=1/2\sum_{k\in I}q^*_k$

$$
R^2=\dfrac{3\sum_{k\in I}q_k^*}{m+2\epsilon^2}
$$

Suponiendo que cada coeficiente está acotado superiormente por $q_k$, luego $R^2\simeq\dfrac{3Km}{m+2\epsilon^2}$, siendo $K=\mathtt{sup}\{q_k^*\}_{k\in I}$

Un resultado similar se obtiene si $\xi\sim N(0,\sigma)$

$$
R^2=\dfrac{3\sum_{k\in I}q_k^*}{m+6\sigma^2}
$$

Usando nuevamente el supremo $K$, tenemos $R^2\simeq \dfrac{3mK}{m+6\sigma^2}$

